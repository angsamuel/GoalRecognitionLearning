%
% FIX THIS -- remove/change, just some examples of things
%
\chapter{Game Model}
We begin by describing our settings and introducing the  
GTGR and GRGRD models. 

\subsection{Game-theoretic goal recognition model}\label{sec:game_basic}
Consider a deterministic environment such as the one in the introduction. 
We can model the environment with a graph 
in which the nodes correspond to the states  
and the edges connect neighboring states. 
Given the environment and the graph, as in many standard GR problems, 
the agent wants to plan out a sequence of moves (i.e., determining a path) 
to reach its target location of the graph.
The target location is unknown to the observer, and the observer's
goals are to identify the target location based on the observed sequence of moves 
and to make preventive measure to protect the target location. 
%In such scenario, it is clear that that the adversary's plan and
%the planner's move depend on each other. 
%To understand the behavior of the agents in a stable state, 
%We use game theory to model such scenario of high interdependence. 

We model this scenario as a two-player zero-sum game, between the agent/ adversary
and the observer. Given the graph $G = (L, E)$ of the environment, 
the adversary is interested in a set of potential targets $B \subseteq L$ 
and has a starting position $s_0 \in L \setminus B$. 
The adversary's aim is to attack a specific target $\theta \in B$, which 
is chosen at random according to some prior probability distribution $P$. 
The observer does not know the target $\theta$, and only the adversary knows its target $\theta$.  
However, the observer knows the set of possible targets $B$ and the adversary's starting position $s_0$. 
For any $s \in L$, we let $\nu(s)$ is the set of neighbors of $s$ in the graph $G$. 

The game is sequential and is played over several timesteps 
where both of the players move simultaneously. 
At each timestep, the observer selects a potential target in $B$ to protect, 
and the agent moves from its current position to a neighboring node.
We consider the zero-sum scenario: With each timestep, 
the adversary and the observer will lose and gain a value $d$, respectively.
In addition, if the observer protects the correct target location $\theta$,
an additional value of $q$ will be added to the observer 
and subtracted from the adversary. 
The game ends when the attacker reaches its target $\theta$, 
a value of $u^{\theta}$ will be added to the adversary's overall score, and $u^{\theta}$ will be subtracted 
from the observer's overall score. 
Notice that during the play of the game, the adversary does not observe 
the observer's action(s), and the players do not know of their current scores. 

Because of the potentially stochastic nature of adversary's moves 
at each timestep and the uncertainty of adversary's target in the system, 
our setting is most naturally modeled as a 
\emph{stochastic game with incomplete information} 
as defined in Section \ref{stochastic_games}.
More specifically, the set of states is $L$ with an initial state $s_0$.
Given a state $s \in S$, $\nu(s)$ is the action set 
for the adversary and $B$ is the action set for the 
observer. Given a state $s \in S$ and $j \in \nu(s)$, 
the single-controller transition function $\chi(s,j) = j$. 
Indeed, the transition between states are controlled 
by the adversary only and is deterministic: From state $s$,
where $s\neq \theta$, given attacker action $j \in \nu(s)$,
the next state is $j$. %, where $\nu(s)$ is the set of neighbors of $s$. 
The state $\theta$ is terminal: Once reached, the game ends.
Given a state $s \in S$, $j \in \nu(s)$, and $i \in B$, 
%a reward function $r(s,i,j) \in \mathbb{R}$. 
%which is a multi-agent generalization of Markov decision (MDP) processes.
%Furthermore, the uncertainty of adversary's target in the system makes our game a stochastic game
%with incomplete information. 
%Formally, a given state $s$ in the game is solely comprised of the location of the attacker. 
%The action of the observer is denoted by $i\in B$ and the action of the adversary denoted by $j\in L$, 
we define the reward function $r^\theta(s,i,j) \equiv r(s, i, j, \theta)$ from the observer's point of view as
\begin{align}
r(s, i, j, \theta) = \left\{
        \begin{array}{ll}
            d & \quad j \neq \theta \textsl{ \& } i\neq \theta \\
            d+q & \quad j \neq \theta \textsl{ \& } i=\theta \\
            d-u^{\theta} & \quad j=\theta \textsl{ \& } i\neq \theta \\
           d+q-u^{\theta} & \quad j=\theta \textsl{ \& } i=\theta. \\
        \end{array}
    \right.
\end{align}
While, in theory, the game could go on forever if the adversary never reaches his target $\theta$, 
because of the per-timestep cost of $d$, any sufficiently long path for the adversary would be 
dominated by the strategy of taking the shortest path to $\theta$. 
Eliminating these dominated strategies allows us to set a finite bound for the duration of the game, 
which grows linearly in the shortest distance to the target that is furthest away. Even in games where the value of $d$ is set to 0, the defender could potentially play a uniformly random strategy that imposes a cost of $\frac{q}{| B |}$ per timestep. Therefore, an adversary strategy taking forever would achieve a value of $-\infty$ against the uniformly random defender strategy. In any Nash equilibrium the attacker will always reach their target in finite time.

We call this the game-theoretic goal recognition (GTGR) model. 
All of the definitions in Section \ref{stochastic_games} follow immediately for our games. 

\subsection{Game-theoretic goal recognition design model}
As mentioned in the introduction, we also consider the 
game-theoretic goal recognition design (GTGRD) model.
Formally, before the game starts, we allow the observer to blocking a subset 
of at most $K$ actions from the game. In our model, that corresponds to blocking at most $K$ edges from the graph.
In one variant of the model, blocking an edge effectively removes that edge, i.e. the adversary can no longer take that action. 
In another variant, blocking an edge does not prevent the adversary from taking the action,
but the adversary would incur a cost by taking that action.
After placing the blocks, the game proceeds as described in Section \ref{sec:game_basic}.
\nocite{Dijkstra80}
\nocite{plop03-paper}
