 def train_agent_against_observer(self, episodes, observer):
      #self.generateQTables(self.gs.nodesNum, .8, self.gs.targets)
      self.scores_dict = dict()
      self.reset_guess_table()
      for t in self.gs.targets:
        self.scores_dict.update({t: []})

      for i in range(0, episodes):
        observer.reset_belief()


        target = self.gs.targets[np.random.randint(len(self.gs.targets))]
        self.refresh_R_table(target, self.gs.targetUtility)

        current_state = self.gs.startState
        prev_state = current_state
        steps = [current_state]
        q_table = self.q_table_dict[target]
        while current_state != target:
          #print(current_state)
          #print(target)
          next_step_index = self.gimme_move(q_table, prev_state, current_state)

          #self.update(current_state,next_step_index,target)
          steps.append(next_step_index)
          
          
          
          #observer work
          observer.update_belief(current_state)
          observer_guess = observer.belief_guess()

          score = self.update(current_state,next_step_index, target)
          if observer_guess == target and current_state != self.gs.startState:
              self.apply_guess_penalty(prev_state, current_state, target)

          self.scores_dict[target].append(score)

          observer.observe_action(current_state, target)
          prev_state = current_state
          current_state = next_step_index

      #
      #for target in self.gs.targets:
      #  plt.plot(self.scores_dict[target])
      #  plt.show()

      #testing
      for target in self.gs.targets:
        # Testing
        observer.reset_belief()
        agent_points = self.gs.targetUtility
        current_state = self.gs.startState
        prev_state = current_state

        steps = [current_state]
        q_table = self.q_table_dict[target]
        while current_state != target:
            #print(current_state)
            #print(target)
            next_step_index = self.gimme_move(q_table, prev_state, current_state)
            steps.append(next_step_index)

            #observer work
            observer.update_belief(current_state)
            observer_guess = observer.belief_guess()

            score = self.update(current_state,next_step_index, target)
            if observer_guess == target and current_state != self.gs.startState:
              self.apply_guess_penalty(prev_state, current_state, target)
              agent_points -= self.gs.guessReward

            observer.observe_action(current_state, target)
            prev_state = current_state
            current_state = next_step_index
        print("Most efficient path:")
        print(steps)
        print("agent score", agent_points)
        print("observer score", self.gs.targetUtility - agent_points)
        self.path_dict.update({target: steps})
        plt.plot(self.scores_dict[target])
        plt.show()


        # Testing
        
      

